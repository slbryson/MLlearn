{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slbryson/MLlearn/blob/master/Para_WhisperGPUTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XP0HfJFqAieJ",
        "outputId": "a41551ec-c5e8-4aa7-813c-8c7e29b81534"
      },
      "outputs": [],
      "source": [
        "#Grab the whisper library\n",
        "!pip install -U openai-whisper\n",
        "!pip install git+https://github.com/openai/whisper.git "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io.wavfile import write\n",
        "import scipy.io.wavfile as wavfile\n",
        "import multiprocessing\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESg7jpqv_Qco",
        "outputId": "a10e03bc-c3fc-4406-f6bc-3822354c6bd3"
      },
      "outputs": [],
      "source": [
        "\n",
        "import whisper\n",
        "\n",
        "# Load the large model\n",
        "model_type = 'medium'\n",
        "model1 = whisper.load_model(model_type)\n",
        "model2 = whisper.load_model(model_type)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gtts import gTTS \n",
        "\n",
        "# The text that you want to convert to audio\n",
        "mytext = 'Hello, welcome to my channel. I hope you are doing well. '\n",
        "language = 'en'\n",
        "speech = gTTS(text = mytext, lang = language, slow = False)\n",
        "\n",
        "# Saving the converted audio in a mp3 file named\n",
        "speech.save(\"random_audio.mp3\")\n",
        "# Saving the converted audio in a wav file named random_audio.wav\n",
        "speech.save(\"random_audio.wav\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGol8x0R--4P",
        "outputId": "7ba053d1-f7c2-41d2-c63c-9891b6c2020e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    # Use the first available GPU device\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    # Print the current device\n",
        "    print(f\"Using device: {torch.cuda.get_device_name(device)}\")\n",
        "elif torch.backends.mps.is_built():\n",
        "    # this ensures that the current MacOS version is at least 12.3+\n",
        "    print(torch.backends.mps.is_available())\n",
        "    # this ensures that the current current PyTorch installation was built with MPS activated.\n",
        "    print(torch.backends.mps.is_built())\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"Using MPS device\")\n",
        "\n",
        "else:\n",
        "    print(\"CUDA is not available, using CPU instead.\")\n",
        "    device = torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S51AsHrD-y7p"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def benchmark_model(model, input_tensor, device, num_iterations):\n",
        "    # Move the model to the current device\n",
        "    model.to(device)\n",
        "\n",
        "    # Move the input tensor to the current device\n",
        "    input_tensor = input_tensor.to(device)\n",
        "\n",
        "    # Run the model once to initialize the GPU\n",
        "    _ = model.transcribe(input_tensor)\n",
        "\n",
        "    # Benchmark the GPU by running the model multiple times\n",
        "    times = []\n",
        "    for i in range(num_iterations):\n",
        "        start_time = time.time()\n",
        "        _ = model.transcribe(input_tensor)\n",
        "        end_time = time.time()\n",
        "        times.append(end_time - start_time)\n",
        "\n",
        "    # Calculate the average time per iteration\n",
        "    avg_time_per_iteration = sum(times) / num_iterations\n",
        "\n",
        "    # Print the average time per iteration\n",
        "    print(f\"Average time per iteration: {avg_time_per_iteration:.5f} seconds\")\n",
        "\n",
        "    # Return the average time per iteration\n",
        "    return avg_time_per_iteration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8sNR1RH_eVH"
      },
      "source": [
        "Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmT5bsmL_S2Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Set the Length of the audio\n",
        "audio_len = 5\n",
        "\n",
        "# Load the audio file using whisper\n",
        "input_tensor = whisper.load_audio('random_audio.mp3')\n",
        "input_tensor = whisper.pad_or_trim(input_tensor)\n",
        "# Reshape the input tensor to have shape (1, num_samples, num_channels)\n",
        "# input_tensor = input_tensor.reshape((1, -1, 1))\n",
        "\n",
        "# Move the input tensor to the current device\n",
        "input_tensor = torch.from_numpy(input_tensor).to(device)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5BDwVYvLAq4",
        "outputId": "ea75aa7b-fbdb-48b0-b781-292ee9e90857"
      },
      "outputs": [],
      "source": [
        "# Check the Input\n",
        "print(input_tensor.shape)\n",
        "# Get the output from model1 and model2 and print the results from both the models \n",
        "# Set the environment variable PYTORCH_ENABLE_MPS_FALLBACK=1 to enable MPS fallback\n",
        "# Set the environment variable PYTORCH_ENABLE_MPS=1 to enable MPS\n",
        "# Set the environment variable PYTORCH_ENABLE_MPS=0 to disable MPS\n",
        "# Set the environment variable PYTORCH_ENABLE_MPS_FALLBACK=0 to disable MPS fallback\n",
        "\n",
        "#python set an environment variable in MacOS \n",
        "import os\n",
        "os.environ[\"PYTORCH_ENABLE_MPS\"] = \"0\"\n",
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
        "# print the environment variable PYTORCH_ENABLE_MPS\n",
        "print(os.environ[\"PYTORCH_ENABLE_MPS\"])\n",
        "#print the environment variable PYTORCH_ENABLE_MPS_FALLBACK \n",
        "print(os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSSwcAr9Oc4u",
        "outputId": "dfb8be67-39a7-46cb-c6ad-c98d968134c6"
      },
      "outputs": [],
      "source": [
        "result = benchmark_model(model2,input_tensor,device,1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HJ3f7yKXx-0",
        "outputId": "d33953bd-c336-40b2-bfc1-0ff30626f737"
      },
      "outputs": [],
      "source": [
        "avg_time = benchmark_model(model1,input_tensor,device,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw9MuPqaY1Km",
        "outputId": "32bcdfc7-69d6-4a7e-e4f1-fa834515918a"
      },
      "outputs": [],
      "source": [
        "inputs= [model1,input_tensor,device,2]\n",
        "# avg1 =[]\n",
        "with multiprocessing.Pool(processes=2) as pool:\n",
        "  avg1 = pool.imap(benchmark_model,inputs)\n",
        "\n",
        "# print(avg1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(avg1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzpNFaCH_aGz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Number of Iterations\n",
        "num_iter = 1\n",
        "# Create two processes to run the models in parallel\n",
        "process1 = multiprocessing.get_context(\"spawn\").Process(target=benchmark_model, args=(model1, input_tensor, device, num_iter))\n",
        "process2 = multiprocessing.get_context(\"spawn\").Process(target=benchmark_model, args=(model2, input_tensor, device, num_iter))\n",
        "\n",
        "# Start the processes\n",
        "process1.start()\n",
        "process2.start()\n",
        "\n",
        "# Wait for the processes to finish\n",
        "process1.join()\n",
        "process2.join()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWjgQbzUeGqO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNMbGx0qnwl611sqhr7vh+A",
      "history_visible": true,
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
